%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required

%
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass{llncs}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{EEG-Based Brain Disorders Diagnosis through Deep Neural Networks%\thanks{Grants or other notes
%about the artBrain Disorders Diagnosis Automation using Deep Neural Networksicle that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
%\subtitle{this is a  subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Gabriel A. Maggiotti (gmaggiotti@gmail.com)
\institute{Jampp.com}
}

%\authorrunning{Short form of author list} % if too long for running head


\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle


\begin{abstract}
In most cases, the diagnosis of brain disorders such as epilepsy or a tumor is slow and requires endless visits to doctors and electroencephalogram (EEG) technicians. This project aims to automate brain disorder diagnosis by using Artificial Intelligence and deep learning. There are many brain disorders  can be detected by reading an Electroencephalography. Using an EEG device and collecting the electrical signals directly from the brain with a noninvasive procedure gives significant information about its health. Classifying and detecting anomalies on these signals is what doctors currently do when reading an Electroencephalography. With the right amount of data and the use of machine learning models, it could be possible to learn and classify these signals into groups like (i.e: anxiety, epilepsy spikes, abnormal tumor activity, etc). Subsequently, a trained Neural Network would interpret those signals and identify evidence of a disorder to automate the detection and classification of those disorders found. 

\keywords{Brain Disorders, EEG, Deep Neural Networks}
\end{abstract}


\paragraph{}\paragraph{}

\section{Introduction}
\label{intro}

This paper explores the use of a supervised machine learning approach to automate the detection of specific disorders on the brain by reading the EEG signals. Primarily, it focuses on Epilepsy and abnormal tumor activities.  Further research could extrapolate this approach to other brain conditions.

\paragraph{}
Epilepsy is a chronic disorder caused by an imbalance in the electrical activity of neurons in one or several areas of the brain. In most epilepsies, an anomaly in electrical activity can be observed through EEG by registering spikes in the affected areas.  These spikes have a unique pattern that can be seen with the naked eye on an electroencephalogram (spikes or peaks are registered with some frequency associated in the amplitudes of the electrical signals recorded).  Also tumors presents a unique pattern in the affected area that can be observed by an EEG.

\paragraph{}
These marks are indicators of the presence of the disorder. Patients carry this pattern of spikes almost all the time. Seizures or epileptic seizures are events of short duration, being the spikes the catalysts thereof.
\paragraph{}

\begin{figure}[h]
\centering
\includegraphics[width=6.08cm,height=4.36cm]{media/eeg-spike.eps}
\caption{Representative abnormal EEG waveforms.}
\end{figure}


This anomalous brain activity generates an observable mark or pattern. That footprint can be learned through a deep neural network. The following section will elaborate the whole process of data extraction and processing, as well as, the proposed five layers of fully connected Neural Network architecture for feature extraction. Furthermore, the process of training the network with a training-set followed by a validation of the results using a testing/validation set. 

\paragraph{}\paragraph{}

\section{Related work on the subject}
\label{sec:1}

In the past, similar studies have been conducted using EEG datasets to analyze and make predictions on epilectic seizures and tumor related activity. 
The following are the most relevant works on the subject (taken from $[$4$]$). The most common classifier 
used was support vector machine (SVM) and for dataset the CHB-MIT 
database. 


\paragraph{}
Related references using ANN with poor results $[$5$]$, $[$6$]$, 
$[$11$]$. No works found on the subject related with the use of deep neural networks.

\begin{table}[h!]
\begin{tabular}{ |p{1.6cm}|p{10cm}|p{1.0cm}|  }

 \hline
 Author     & & \\
 \hline
 Webber &  $[$5$]$ - ANN classification system SEN of 76\% and FPR of 1 event/h &1996 \\
 Liu&  $[$22$]$ - Wavelet decomposition-based feature extraction  and by SVM  SEN of 94.5\% and SPEC of 95.3\% & 2012 \\
Direito& $[$23$]$ - Markov modeling classification system. Identified four states - accuracy of 89.3\% & 2012 \\
Rabbi& $[$24$]$ - Used fuzzy algorithms for feature extraction for classification SEN of 95.8\%  & 2012 \\
Thodorof, Pineau, Lim& $[$25$]$ - recurrent convolutional architecture designed to capture spectral, temporal and spatial patterns representing a seizure  & 2016 \\
 \hline
\end{tabular}
\caption{ The accuracy of the validation set, the training error and the number of iterations for each model and dataset.}
\end{table}

\paragraph{}
\paragraph{}
\paragraph{}



\paragraph{}
\section{Methodology: Dataset Processing}
\label{sec:2}

Dataset used was taken from The University of California Irvine [1][2].  UCI contains an Epileptic Seizure Data Set supported by 11500 measurements from a total of 500 individuals with each has 4097 data points for 23.5 seconds and sampling rate of the data was 173.61 Hz. Then divided and shuffled every 4097 data points into 23 chunks, each chunk contains 178 data points for 1 second, and each data point is the value of the EEG recording at a different point in time. So now we have 23 x 500 = 11500 pieces of information(rows), each information contains 178 data points for 1 second(columns), the last column represents the labels.  The dataset contains five different classes of 2300 samples each. Labels 1,2 and 5 were used respectively: class (1 for seizure activity; class (2 for abnormal tumor activity and class (5 for patients without seizures.  Finally, two dataset were constructed.   For epileptic seizures samples of classes 1 and 5 were used (4600 samples).  And for tumor  activity classes 2 and 5 were used with the same number of samples.
   
\paragraph{}

To avoid saturation on the activation function and to make the gradient descent converge faster, the features were normalized to a range of values between -1 and 1 so that all features have a similar scale. The method used was standardization, which makes every feature have a zero mean value and unit variance. It is calculated for each feature as follows:


\begin{equation} 
x'=\frac{x-\hat{x}}{\sigma}
\end{equation}

\begin{equation}
\mu (x_{i})= 0   
\end{equation}

\begin{equation} 
\sigma (x_{i}) = \sigma(x_{j})
\end{equation}


\begin{figure}[h]
\centering
\includegraphics[width=9.81cm,height=5.00cm]{media/image7.eps}
\caption{Feature scaling is a method used to standardize the 
range of independent variables or features of data. In data processing, 
it is also known as data normalization and is generally performed during 
the data preprocessing step.}
\end{figure}

\paragraph{}
\paragraph{}
\paragraph{}

Lastly, the dataset will be further split into training, test and validation sets. It is very important that dataset is shuffled well to avoid any element of bias before training the ML model.

\paragraph{}
\paragraph{}
\paragraph{}

\section{Method / The Solution}
\label{sec:3}
Deep learning algorithms are composed of multiple processing layers that learn data representations with multiple levels of abstraction. Using a deep learning network (DNN) implemented in Python (TensorFlow library), we classified the subjects based on each label.  Design a fully connected Neural Network to capture the nonlinearity of the signals.
 
\paragraph{}
The proposed architecture consists of five layers of fully connected neural networks (Fig. 3) to capture data nonlinearity. An Adam optimizer[51] was used because it is an efficient extension of stochastic gradient descent optimizers. The Adam optimizer achieves good results faster than other approaches and is used for objective function minimization by iteration. It computes individual adaptive learning rates from estimates of the first and second moments of the gradients.  A reference to the project and code can be found at [50]. 

\paragraph{}
Parameter initialization included assigning random values between 0 and 1 to the weights and zero values to the biases. Also, Xavier initialization was applied to the weights following Eq. 1 to obtain a global minimum of the cost function faster and more efficiently: 

\begin{equation} 
\theta\Rightarrow\theta=\{W_{0},W_{1},W_{2}...,W_{L}\}
\end{equation}

\begin{equation} 
Xavier = \sqrt{\frac{2}{features}}
\end{equation}

\paragraph{}
The weights were still random, but positive and negative values close to 0 were assigned to produce outputs that followed a similar distribution across all neurons. 

\paragraph{}
The nonlinear sigmoid function was applied as the activation function of hidden layers. The objective function used measures the error between the neural networks output and the actual target, as shown in Eq. 7: 

\begin{figure}[h]
\centering
\includegraphics[width=10.51cm,height=4.77cm]{media/deep-nn1.eps}
\caption{Architecture for a four five fully connected 
Neural Network}
\end{figure}


\paragraph{}Iterate for N epochs,  for each training example Xi, Yi 
\begin{equation} 
g(x)^{i+1}=\sum_j^n(x_{j}*w_{j})\Rightarrow X^{i}*W^{i}
\end{equation}

\paragraph{}
Hidden activation layers are components that introduce non-linearity to 
the system. That Allows to capture and perform very sophisticated type 
of classification functions.
\begin{equation} 
L^{i+1}=sigmoid(g(x)^{i+1})
\end{equation}

\paragraph{}
\paragraph{}Calculate the error comparing the output of the NN with the actual target 
\begin{equation} 
Error = \frac{1}{2}\sum_i^n( y -\widehat{y})^2
\end{equation}

\begin{equation} 
\widehat{y}=Sigmoid(x_{i}\times w_{i})
\end{equation}

\paragraph{}
\paragraph{} Use the chain rule to efficiently compute gradients, top to bottom

\begin{equation} 
\frac{\partial E}{\partial w}=\frac{\partial }{\partial w} \frac{1}{2}\sum_i^n( y -\widehat{y})^2
\end{equation}

\begin{equation} 
\frac{\partial E}{\partial w} = \sum_i^n ( y -\widehat{y})  (-\frac{\partial E}{\partial w}\widehat{•}t{y})
\end{equation}

\begin{equation} 
\Rightarrow(\frac{\partial E}{\partial w}\widehat{y})= \widehat{y}(1-\widehat{y})
\end{equation}

\paragraph{}
\paragraph{}Back propagation of errors using the chain rule
\begin{equation} 
\nabla=\frac{\partial E}{\partial w}
\end{equation}

\begin{equation} 
\nabla_{n-1}=\nabla{n}*W^{T}_{n-1}
\end{equation}

\paragraph{}
\paragraph{}
As a regularization procedure for avoiding overfitting, a dropout approach was employed in the fourth hidden layer with a keep probability of 0.5[60]. The optimization procedure was iterated until the minimum error on the training set and the maximum accuracy on the validation set (the number of observations that were correctly classified) were reached (Fig. 4). 
\paragraph{}

\begin{figure}[h]
\centering
\includegraphics[width=6.08cm,height=5.11cm]{media/training-pro.eps}
\caption{ Training process}
\end{figure}

\paragraph{}
\section{Results}
\label{sec:4}
 The main model used in the experiments was a five-layer fully-connected neural networks with a learning rate of 0.0001, Xavier parameter of 0.8, dropout with a keep probability of 50\%, L2 regularization with a beta of 0.0001, sigmoid activation and exponential decay. 

\paragraph{}
The model was trained with two datasets, one with patients who had a tumor, for whom brain activity was collected in the affected area. The second group was made up of  patients with epileptic seizures.
\paragraph{}
On the UCI dataset,a three-class classification task was performed. The first group, group A, was comprised of  2300 samples of healthy recordings. The second, group B, was a set of 2300 samples of tumor activity recording, The same approach was implemented for epilepsy,  where a set of recordings with epileptic seizures, group  C. Then two datasets were created, A + B for tumor classification and A + C for epileptic seizure classification. Both datasets were shuffled and the data was normalized. For each dataset 80\% was taken for training and the remaining 20\% for validation. 

\paragraph{}

\begin{table}[h!]
\begin{tabular}{ |p{3cm}||p{2cm}|p{2cm}|p{2cm}|p{1.5cm}|  }
 \hline
 \multicolumn{5}{|c|}{Country List} \\
 \hline
 Model     & Dataset &Accuracy \%&Error \%& Number of Iterations\\
 \hline
 3 Layer NN & Epilepsy &97.06 +/- 0.14& 1.2 +/- 0.9 &300\\
 5 Layer NN & Epilepsy  & 99.69 +/- 0.05   &0.3 +/- 0.6 &300\\
 5 Layer NN & Tumor & 85.04 +/- 0.08 &  2.99 +/- 0.16 &2760\\
 \hline
\end{tabular}
\caption{ The accuracy of the validation set, the training error and the number of iterations for each model and dataset.}
\end{table}



\begin{figure}[h]
\centering
\includegraphics[width=12.08cm,height=5.11cm]{media/results.eps}
\caption{ Error using 1 layer and 5 layers fully connected Neural Networks trained through 1000 epochs. 1 layer error:  0.175971;  5 layers error:  0.012657}
\end{figure}



\begin{figure}[h]
\centering
\includegraphics[width=12.08cm,height=5.11cm]{media/results2.eps}
\caption{ Error using 1 layer and 5 layers fully connected Neural Networks trained through 1000 epochs. 1 layer error:  0.175971;  5 layers error:  0.012657}
\end{figure}



 

\paragraph{}\paragraph{}
\paragraph{}\paragraph{}
\paragraph{}\paragraph{}
\paragraph{}\paragraph{}


\section{Conclusion and Future Directions}
\label{sec:4}



 A successful automated detection and prediction of disorders introduces new innovative opportunities for diagnosis and preventive health care. This paper proposes a fast and lightway learning procedure for building a predictive model that satisfies the assignment. The use of deep neural networks in the subject turned out to be an excellent solution that presents high accuracy.  
\paragraph{}
The results are prominent and suggest that the model with existing clinical systems and practices may enable clinicians to make accurate epilepsy diagnosis and start  treatments earlier.
\paragraph{}
Moreover, it opens a door to extend the work on other areas like diagnosis of dementia, brain damage, brain diseases, psychiatric disorders, tumors, stroke, seizure forecasting from the study of interictal, preictal and ictal states and other focal brain disorders. 
\paragraph{}
Another area of interest would be Electrocardiogram signals. Further works can also be done on predicting heart attacks from ECG signals (people carrying holter monitors). 





% Non-BibTeX users please use
\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
\bibitem{RefJ}



$[$1$]$ - \underline{https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition}
$[$2$]$ - Andrzejak RG, Lehnertz K, Rieke C, Mormann F, David P, Elger 
CE (2001) Indications of nonlinear deterministic and finite dimensional 
structures in time series of brain electrical activity: Dependence on 
recording region and brain state, Phys. Rev. E, 64, 061907

$[$3$]$ - A.H. Shoeb, Application of Machine Learning to Epileptic 
Seizure Onset and Treatment, 2009.

$[$4$]$ - 
https://www.sciencedirect.com/science/article/pii/S1525505014002297

$[$5$]$ W.R. Webber, R.P. Lesser, R.T. Richardson, K. Wilson - An 
approach to seizure detection using an artificial neural network (ANN)

$[$6$]$ N. Pradhan, P.K. Sadasivan, G.R. Arunodaya - Detection of 
seizure activity in EEG by an artificial neural network: a preliminary 
study

$[$7$]$ A.J. Gabor -Seizure detection using a self-organizing neural 
network: validation and comparison with other detection strategies

$[$8$]$ S.B. Wilson, M.L. Scheuer, R.G. Emerson, A.J. Gabor - Seizure 
detection: evaluation of the Reveal algorithm

$[$9$]$ S.B. Wilson - A neural network method for automatic and 
incremental learning applied to patient-dependent seizure detection

$[$10$]$ M. D'Alessandro, G. Vachtsevanos, R. Esteller, J. Echauz, S. 
Cranstoun, G. Worrell, et al. - A multi-feature and multi-channel 
univariate selection process for seizure prediction

$[$11$]$ A. Aarabi, F. Wallois, R. Grebe - Automated neonatal seizure 
detection: a multistage classification system through feature selection 
based on relevance and redundancy analysis

$[$12$]$ A.M. Chan, F.T. Sun, E.H. Boto, B.M. Wingeier - Automated 
seizure onset detection for accurate onset time determination in 
intracranial EEG

$[$13$]$ T. Netoff, Y. Park, K. Parhi - Seizure prediction using 
cost-sensitive support vector machine

$[$14$]$ K.C. Chua, V. Chandran, U.R. Acharya, C.M. Lim - Automatic 
identification of epileptic electroencephalography signals using 
higher-order spectra

$[$15$]$ P. Mirowski, D. Madhavan, Y. Lecun, R. Kuzniecky - 
Classification of patterns of EEG synchronization for seizure prediction

$[$16$]$ T.L. Sorensen, U.L. Olsen, I. Conradsen, J. Henriksen, T.W. 
Kjaer, C.E. Thomsen, et al. - Automatic epileptic seizure onset 
detection using matching pursuit: a case study

$[$17$]$ L. Chisci, A. Mavino, G. Perferi, M. Sciandrone, C. Anile, G. 
Colicchio, et al. - Real-time epileptic seizure prediction using AR 
models and support vector machines

$[$18$]$ E.B. Petersen, J. Duun-Henriksen, A. Mazzaretto, T.W. Kjaer, 
C.E. Thomsen, H.B. Sorensen - Generic single-channel detection of 
absence seizures

$[$19$]$ A. Temko, E. Thomas, W. Marnane, G. Lightbody, G. Boylan - 
EEG-based neonatal seizure detection with Support Vector Machines

$[$20$]$ U.R. Acharya, S.V. Sree, J.S. Suri - Automatic detection of 
epileptic EEG signals using higher order cumulant features

$[$21$]$ A. Kharbouch, A. Shoeb, J. Guttag, S.S. Cash - An algorithm for 
seizure onset detection using intracranial EEG

$[$22$]$ Y. Liu, W. Zhou, Q. Yuan, S. Chen - Automatic seizure detection 
using wavelet transform and SVM in long-term intracranial EEG

$[$23$]$ B. Direito, C. Teixeira, B. Ribeiro, M. Castelo-Branco, F. 
Sales, A. Dourado - Modeling epileptic brain states using EEG spectral 
analysis and topographic mapping

$[$24$]$ A.F. Rabbi, R. Fazel-Rezai - A fuzzy logic system for seizure 
onset detection in intracranial EEG

$[$50$]$ "source code" - https://github.com/gmaggiotti/brain-disorders-prediction





\end{thebibliography}




\end{document}
